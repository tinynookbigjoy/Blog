#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hugoåšå®¢å¤‡ä»½å·¥å…· - æœ€ç»ˆç‰ˆæœ¬

åŠŸèƒ½ï¼š
- ä»Hugoåšå®¢æå–Markdownå†…å®¹å’Œå›¾ç‰‡èµ„æº
- æ™ºèƒ½å¢é‡å¤‡ä»½
- è·¯å¾„ä¿®æ­£å’Œæ ¼å¼åŒ–
- ç”Ÿæˆç»“æ„åŒ–README
"""

import os
import shutil
import re
import hashlib
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any
from dataclasses import dataclass

@dataclass
class ArticleInfo:
    """æ–‡ç« ä¿¡æ¯æ•°æ®ç±»"""
    title: str
    filename: str
    path: str
    year: int
    month: int
    day: int
    date_str: str

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "config.json"):
        self.config_path = config_path
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
        except json.JSONDecodeError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
    
    def get(self, key_path: str, default=None):
        """è·å–é…ç½®é¡¹ï¼Œæ”¯æŒç‚¹å·åˆ†éš”çš„è·¯å¾„"""
        keys = key_path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value

class HugoBlogBackup:
    """Hugoåšå®¢å¤‡ä»½ä¸»ç±»"""
    
    def __init__(self, config_path: str = "config.json"):
        self.config = ConfigManager(config_path)
        self._init_paths()
        self._init_filters()
        
        # å­˜å‚¨æ–‡ç« ä¿¡æ¯å’Œä½¿ç”¨çš„å›¾ç‰‡
        self.articles = {}
        self.used_images = set()
        
        # å­˜å‚¨å˜æ›´ä¿¡æ¯
        self.updated_files = []
        
        # åˆå§‹åŒ–æ–‡ç« åˆ†ç±»
        categories = self.config.get('readme.categories', {})
        for category in categories.keys():
            self.articles[category] = []
    
    def _init_paths(self):
        """åˆå§‹åŒ–è·¯å¾„é…ç½®"""
        self.source_root = Path(self.config.get('paths.source_root')).expanduser().resolve()
        self.backup_root = Path(self.config.get('paths.backup_root')).expanduser().resolve()
        
        # æ„å»ºå®Œæ•´è·¯å¾„æ˜ å°„
        self.source_dirs = {}
        self.target_dirs = {}
        
        source_dirs_config = self.config.get('paths.source_dirs', {})
        target_dirs_config = self.config.get('paths.target_dirs', {})
        
        for key in source_dirs_config:
            self.source_dirs[key] = self.source_root / source_dirs_config[key]
            self.target_dirs[key] = self.backup_root / target_dirs_config[key]
    
    def _init_filters(self):
        """åˆå§‹åŒ–è¿‡æ»¤å™¨é…ç½®"""
        self.ignore_files = set(self.config.get('filters.ignore_files', []))
        self.markdown_extensions = set(self.config.get('filters.markdown_extensions', ['.md']))
    
    def log(self, message: str):
        """æ—¥å¿—è¾“å‡º"""
        if self.config.get('logging.verbose', True):
            print(message)
    
    def should_ignore_file(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«å¿½ç•¥"""
        return filename in self.ignore_files
    
    def is_markdown_file(self, filename: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºmarkdownæ–‡ä»¶"""
        return any(filename.lower().endswith(ext) for ext in self.markdown_extensions)
    
    def get_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""
    
    def file_needs_update(self, source_file: Path, target_file: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦éœ€è¦æ›´æ–°"""
        if not target_file.exists():
            return True
        
        source_hash = self.get_file_hash(source_file)
        target_hash = self.get_file_hash(target_file)
        
        return source_hash != target_hash
    
    def extract_frontmatter_title(self, content: str) -> str:
        """ä»frontmatterä¸­æå–æ ‡é¢˜"""
        frontmatter_pattern = r'^---\s*\n(.*?)\n---\s*\n'
        match = re.search(frontmatter_pattern, content, re.DOTALL)
        
        if match:
            frontmatter = match.group(1)
            title_match = re.search(r'title:\s*["\']?(.*?)["\']?\s*$', frontmatter, re.MULTILINE)
            if title_match:
                return title_match.group(1).strip()
        
        return "æœªçŸ¥æ ‡é¢˜"
    
    def extract_image_paths(self, content: str) -> Set[str]:
        """ä»markdownå†…å®¹ä¸­æå–å›¾ç‰‡è·¯å¾„"""
        image_paths = set()
        img_pattern = r'!\[.*?\]\(([^)]+)\)'
        matches = re.findall(img_pattern, content)
        
        path_patterns = self.config.get('images.path_patterns', [])
        
        for match in matches:
            # ç§»é™¤å¯èƒ½çš„é”šç‚¹æ ‡è®°
            clean_path = match.split('#')[0]
            
            for pattern in path_patterns:
                if clean_path.startswith(pattern):
                    relative_path = clean_path[len(pattern):]
                    image_paths.add(relative_path)
                    break
        
        return image_paths
    
    def fix_paths_in_content(self, content: str) -> str:
        """ä¿®æ­£å†…å®¹ä¸­çš„è·¯å¾„"""
        # ä¿®æ­£å›¾ç‰‡è·¯å¾„
        img_corrections = self.config.get('path_corrections.images', {})
        from_patterns = img_corrections.get('from_patterns', [])
        to_pattern = img_corrections.get('to_pattern', '')
        
        for pattern in from_patterns:
            escaped_pattern = re.escape(pattern)
            content = re.sub(
                rf'!\[([^\]]*)\]\({escaped_pattern}([^)]+)\)',
                rf'![\1]({to_pattern}\2)',
                content
            )
        
        # ä¿®æ­£æ–‡ç« å¼•ç”¨è·¯å¾„
        article_corrections = self.config.get('path_corrections.articles', {})
        for old_dir, new_dir in article_corrections.items():
            old_pattern = f'/{old_dir}/'
            escaped_old = re.escape(old_pattern)
            content = re.sub(
                rf'\[([^\]]+)\]\({escaped_old}([^)]*)?(\))',
                rf'[\1]({new_dir}\2\3)',
                content
            )
            
            # å¤„ç†åªæœ‰ç›®å½•çš„å¼•ç”¨
            old_pattern_simple = f'/{old_dir}'
            escaped_old_simple = re.escape(old_pattern_simple)
            content = re.sub(
                rf'\[([^\]]+)\]\({escaped_old_simple}(/[^)]*)?(\))',
                rf'[\1]({new_dir.rstrip("/")}\2\3)',
                content
            )
        
        return content
    
    def process_markdown_content(self, content: str) -> Tuple[str, str, Set[str]]:
        """å¤„ç†markdownå†…å®¹"""
        title = self.extract_frontmatter_title(content)
        image_paths = self.extract_image_paths(content)
        
        # ç§»é™¤frontmatter
        frontmatter_pattern = r'^---\s*\n.*?\n---\s*\n'
        content_without_frontmatter = re.sub(frontmatter_pattern, '', content, flags=re.DOTALL)
        
        # ä¿®æ­£è·¯å¾„
        content_with_fixed_paths = self.fix_paths_in_content(content_without_frontmatter)
        
        # æ·»åŠ æ ‡é¢˜
        processed_content = f"# {title}\n\n{content_with_fixed_paths.strip()}"
        
        return processed_content, title, image_paths
    
    def extract_date_from_filename(self, filename: str) -> Tuple[int, int, int, str]:
        """ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸä¿¡æ¯"""
        date_match = re.match(r'^(\d{4})(\d{2})(\d{2})_', filename)
        if date_match:
            year, month, day = date_match.groups()
            date_format = self.config.get('readme.date_format', 'YYYY-MM-DD')
            if date_format == 'YYYY-MM-DD':
                date_str = f"{year}-{month}-{day}"
            else:
                date_str = f"{year}å¹´{int(month)}æœˆ{int(day)}æ—¥"
            return (int(year), int(month), int(day), date_str)
        return (0, 0, 0, "")
    
    def ensure_target_dirs(self):
        """ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨"""
        for dir_path in self.target_dirs.values():
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def process_markdown_files(self, category: str):
        """å¤„ç†æŒ‡å®šç±»åˆ«çš„markdownæ–‡ä»¶"""
        source_dir = self.source_dirs.get(category)
        target_dir = self.target_dirs.get(category)
        
        if not source_dir or not source_dir.exists():
            self.log(f"âš ï¸  æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
            return
        
        self.log(f"ğŸ“ å¤„ç† {category} æ–‡ä»¶...")
        
        processed_count = 0
        skipped_count = 0
        ignored_count = 0
        
        for file_path in source_dir.rglob('*'):
            if not file_path.is_file() or not self.is_markdown_file(file_path.name):
                continue
            
            if self.should_ignore_file(file_path.name):
                self.log(f"  â­ï¸  å¿½ç•¥æ–‡ä»¶: {file_path.name}")
                ignored_count += 1
                continue
            
            # è®¡ç®—ç›®æ ‡æ–‡ä»¶è·¯å¾„
            rel_path = file_path.relative_to(source_dir)
            target_file = target_dir / rel_path
            target_file.parent.mkdir(parents=True, exist_ok=True)
            
            # è¯»å–æ–‡ä»¶å†…å®¹ç”¨äºå¤„ç†å’Œä¿¡æ¯æå–
            try:
                content = file_path.read_text(encoding='utf-8')
                title = self.extract_frontmatter_title(content)
                image_paths = self.extract_image_paths(content)
                self.used_images.update(image_paths)
                
                year, month, day, date_str = self.extract_date_from_filename(file_path.name)
                
                article_info = ArticleInfo(
                    title=title,
                    filename=file_path.name,
                    path=str(target_file.relative_to(self.backup_root)),
                    year=year,
                    month=month,
                    day=day,
                    date_str=date_str
                )
                self.articles[category].append(article_info)
                
                # å¤„ç†å†…å®¹
                processed_content, _, _ = self.process_markdown_content(content)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ¯”è¾ƒå¤„ç†åçš„å†…å®¹ï¼‰
                needs_update = True
                if target_file.exists():
                    try:
                        existing_content = target_file.read_text(encoding='utf-8')
                        needs_update = existing_content != processed_content
                    except Exception:
                        needs_update = True
                
                if not needs_update:
                    self.log(f"  â†”ï¸  è·³è¿‡æ–‡ä»¶: {file_path.name} (æ— å˜åŒ–)")
                    skipped_count += 1
                else:
                    # å†™å…¥æ–‡ä»¶
                    target_file.write_text(processed_content, encoding='utf-8')
                    self.log(f"  âœ… æ›´æ–°æ–‡ä»¶: {file_path.name} -> {title}")
                    self.updated_files.append(f"{category}/{file_path.name}")
                    processed_count += 1
                
            except Exception as e:
                self.log(f"  âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
        
        self.log(f"  ğŸ“Š {category}: æ›´æ–° {processed_count} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡ {skipped_count} ä¸ªæ–‡ä»¶ï¼Œå¿½ç•¥ {ignored_count} ä¸ªæ–‡ä»¶")
    
    def copy_used_images(self):
        """å¤åˆ¶è¢«ä½¿ç”¨çš„å›¾ç‰‡"""
        source_pics = self.source_dirs.get('pics')
        target_pics = self.target_dirs.get('pics')
        
        if not source_pics or not source_pics.exists():
            self.log(f"âš ï¸  å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {source_pics}")
            return
        
        target_pics.mkdir(parents=True, exist_ok=True)
        
        copied_count = 0
        skipped_count = 0
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        all_images = set()
        for file_path in source_pics.rglob('*'):
            if file_path.is_file():
                rel_path = str(file_path.relative_to(source_pics))
                all_images.add(rel_path)
        
        # åªå¤„ç†è¢«ä½¿ç”¨çš„å›¾ç‰‡
        for image_path in self.used_images:
            source_file = source_pics / image_path
            target_file = target_pics / image_path
            
            if not source_file.exists():
                continue
            
            target_file.parent.mkdir(parents=True, exist_ok=True)
            
            if not self.file_needs_update(source_file, target_file):
                skipped_count += 1
                continue
            
            try:
                shutil.copy2(source_file, target_file)
                self.updated_files.append(f"pics/{image_path}")
                copied_count += 1
            except Exception as e:
                self.log(f"  âŒ å¤åˆ¶å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
        
        unused_count = len(all_images) - len(self.used_images)
        self.log(f"ğŸ–¼ï¸  å›¾ç‰‡åŒæ­¥å®Œæˆ: å¤åˆ¶ {copied_count} ä¸ªï¼Œè·³è¿‡ {skipped_count} ä¸ªï¼Œå¿½ç•¥æœªä½¿ç”¨ {unused_count} ä¸ª")
        self.log(f"   -> {target_pics}")
    
    def generate_readme(self):
        """ç”ŸæˆREADMEæ–‡ä»¶"""
        readme_title = self.config.get('readme.title', 'åšå®¢æ–‡ç« å¤‡ä»½')
        categories_config = self.config.get('readme.categories', {})
        
        readme_content = f"# {readme_title}\n\n"
        
        # æŒ‰é…ç½®çš„é¡ºåºå¤„ç†åˆ†ç±»
        sorted_categories = sorted(categories_config.items(), key=lambda x: x[1].get('order', 999))
        
        for category_key, category_config in sorted_categories:
            category_name = category_config.get('name', category_key)
            readme_content += f"# {category_name}\n\n"
            
            articles = self.articles.get(category_key, [])
            if articles:
                # æŒ‰å¹´ä»½åˆ†ç»„
                articles_by_year = {}
                for article in articles:
                    if article.year > 0:
                        if article.year not in articles_by_year:
                            articles_by_year[article.year] = []
                        articles_by_year[article.year].append(article)
                
                # æŒ‰å¹´ä»½å€’åº
                for year in sorted(articles_by_year.keys(), reverse=True):
                    readme_content += f"## {year}\n\n"
                    
                    # æŒ‰æ—¥æœŸå€’åº
                    year_articles = sorted(
                        articles_by_year[year],
                        key=lambda x: (x.year, x.month, x.day),
                        reverse=True
                    )
                    
                    for article in year_articles:
                        if article.date_str:
                            readme_content += f"* {article.date_str} [{article.title}]({article.path})\n"
                        else:
                            readme_content += f"* [{article.title}]({article.path})\n"
                    
                    readme_content += "\n"
                
                # å¤„ç†æ²¡æœ‰æ—¥æœŸçš„æ–‡ç« 
                no_date_articles = [a for a in articles if a.year == 0]
                if no_date_articles:
                    readme_content += "## å…¶ä»–\n\n"
                    for article in no_date_articles:
                        readme_content += f"* [{article.title}]({article.path})\n"
                    readme_content += "\n"
            else:
                readme_content += "æš‚æ— å†…å®¹\n\n"
        
        # å†™å…¥READMEæ–‡ä»¶
        readme_path = self.backup_root / "README.md"
        readme_path.write_text(readme_content, encoding='utf-8')
        
        self.log(f"âœ… README.md å·²ç”Ÿæˆ: {readme_path}")
    
    def show_statistics(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        if not self.config.get('logging.show_stats', True):
            return
        
        total_articles = sum(len(articles) for articles in self.articles.values())
        
        self.log("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        for category, articles in self.articles.items():
            category_config = self.config.get(f'readme.categories.{category}', {})
            category_name = category_config.get('name', category)
            self.log(f"  - {category_name}: {len(articles)} ç¯‡")
        
        self.log(f"  - æ€»è®¡: {total_articles} ç¯‡")
        self.log(f"  - ä½¿ç”¨çš„å›¾ç‰‡: {len(self.used_images)} ä¸ª")
        
        # æ˜¾ç¤ºå˜æ›´æ±‡æ€»
        if self.updated_files:
            self.log(f"ğŸ”„ æœ¬æ¬¡æ›´æ–°äº† {len(self.updated_files)} ä¸ªæ–‡ä»¶:")
            for file_path in self.updated_files:
                self.log(f"  - {file_path}")
        else:
            self.log("âœ¨ æœ¬æ¬¡è¿è¡Œæ— æ›´æ–°")
    
    def backup(self):
        """æ‰§è¡Œå®Œæ•´çš„å¤‡ä»½æµç¨‹"""
        self.log("ğŸš€ å¼€å§‹Hugoåšå®¢å¤‡ä»½...")
        self.log(f"æºè·¯å¾„: {self.source_root}")
        self.log(f"å¤‡ä»½è·¯å¾„: {self.backup_root}")
        self.log("-" * 50)
        
        # 1. åˆ›å»ºç›®å½•ç»“æ„
        self.ensure_target_dirs()
        self.log("âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")
        
        # 2. å¤„ç†å„ç±»åˆ«çš„markdownæ–‡ä»¶
        for category in self.source_dirs.keys():
            if category != 'pics':
                self.process_markdown_files(category)
        
        # 3. å¤åˆ¶è¢«ä½¿ç”¨çš„å›¾ç‰‡
        self.copy_used_images()
        
        # 4. ç”ŸæˆREADME
        self.generate_readme()
        
        self.log("-" * 50)
        self.log("ğŸ‰ å¤‡ä»½å®Œæˆ!")
        self.log(f"å¤‡ä»½æ–‡ä»¶ä½ç½®: {self.backup_root}")
        
        # 5. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        self.show_statistics()

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Hugoåšå®¢å¤‡ä»½å·¥å…·')
    parser.add_argument('--config', '-c', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    try:
        backup_tool = HugoBlogBackup(args.config)
        backup_tool.backup()
    except Exception as e:
        print(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())